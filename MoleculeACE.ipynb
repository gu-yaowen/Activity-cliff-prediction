{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MoleculeACE import MPNN, Data, calc_rmse, calc_cliff_rmse, get_benchmark_config\n",
    "from Feature.const import Descriptors\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'CHEMBL2034_Ki'\n",
    "descriptor = Descriptors.GRAPH\n",
    "data = Data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOLECULEACE_DATALIST = ['CHEMBL1862_Ki', 'CHEMBL1871_Ki', 'CHEMBL2034_Ki', 'CHEMBL2047_EC50',\n",
    "                        'CHEMBL204_Ki', 'CHEMBL2147_Ki', 'CHEMBL214_Ki', 'CHEMBL218_EC50',\n",
    "                        'CHEMBL219_Ki', 'CHEMBL228_Ki', 'CHEMBL231_Ki', 'CHEMBL233_Ki',\n",
    "                        'CHEMBL234_Ki', 'CHEMBL235_EC50', 'CHEMBL236_Ki', 'CHEMBL237_EC50',\n",
    "                        'CHEMBL237_Ki', 'CHEMBL238_Ki', 'CHEMBL239_EC50', 'CHEMBL244_Ki',\n",
    "                        'CHEMBL262_Ki', 'CHEMBL264_Ki', 'CHEMBL2835_Ki', 'CHEMBL287_Ki',\n",
    "                        'CHEMBL2971_Ki', 'CHEMBL3979_EC50', 'CHEMBL4005_Ki', 'CHEMBL4203_Ki',\n",
    "                        'CHEMBL4616_EC50', 'CHEMBL4792_Ki']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(MOLECULEACE_DATALIST[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.x_test\n",
    "data(descriptor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.x_test[0].edge_attr = np.zeros(data.x_test[0].edge_attr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import molvs\n",
    "from rdkit import Chem\n",
    "from chemprop.data.utils import get_class_sizes, get_data, get_task_names\n",
    "from chemprop.data import MoleculeDataset, StandardScaler\n",
    "from KANO_model.model import build_model, add_functional_prompt\n",
    "from KANO_model.utils import build_optimizer, build_lr_scheduler, build_loss_func\n",
    "from data_prep import split_data\n",
    "import random\n",
    "import torch\n",
    "from chemprop.train.evaluate import evaluate_predictions\n",
    "import importlib\n",
    "import train_val\n",
    "importlib.reload(train_val)\n",
    "from train_val import predict_epoch, train_epoch, evaluate_epoch\n",
    "from chemprop.train.evaluate import evaluate_predictions\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "args = Namespace(activation='ReLU', atom_messages=False, batch_size=256,\n",
    "                 bias=False, checkpoint_dir=None,\n",
    "                 checkpoint_path='KANO_model/dumped/pretrained_graph_encoder/original_CMPN_0623_1350_14000th_epoch.pkl',\n",
    "                 checkpoint_paths=['KANO_model/dumped/pretrained_graph_encoder/original_CMPN_0623_1350_14000th_epoch.pkl'],\n",
    "                 config_path=None, crossval_index_dir=None, crossval_index_file=None, cuda=True,\n",
    "                 data_path='data/MoleculeACE/CHEMBL1862_Ki.csv', dataset_type='regression', depth=3, dropout=0.0,\n",
    "                 dump_path='dumped', encoder_name='CMPNN', ensemble_size=1, epochs=100, exp_id='bbbp_test',\n",
    "                 exp_name='finetune', features_generator=None, features_only=False, features_path=None,\n",
    "                 features_scaling=True, ffn_hidden_size=300, ffn_num_layers=2, final_lr=0.0001,\n",
    "                 folds_file=None, gpu=0, hidden_size=300, init_lr=0.0001, log_frequency=10, max_data_size=None,\n",
    "                 max_lr=0.001, metric='r2', minimize_score=True, multiclass_num_classes=3, no_cache=False,\n",
    "                 num_lrs=1, num_runs=1, quiet=False, save_dir=None,\n",
    "                 save_smiles_splits=False, seed=43, separate_test_features_path=None, separate_test_path=None,\n",
    "                 separate_val_features_path=None, separate_val_path=None, show_individual_scores=False,\n",
    "                 split_sizes=[0.8, 0.1, 0.1], split_type='scaffold_balanced', step='functional_prompt',\n",
    "                 temperature=0.1, test=False, test_fold_index=None, undirected=False, use_compound_names=False,\n",
    "                 use_input_features=None, val_fold_index=None, warmup_epochs=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_save_dir(args):\n",
    "    if args.save_dir is None:\n",
    "        args.save_dir = os.path.join('exp_results', args.data_name, str(args.seed))\n",
    "    else:\n",
    "        args.save_dir = os.path.join('exp_results', args.save_dir, str(args.seed))\n",
    "    if not os.path.exists(args.save_dir):\n",
    "        os.makedirs(args.save_dir)\n",
    "    return args\n",
    "\n",
    "def set_seed(random_seed):\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "\n",
    "def check_molecule(smiles):\n",
    "    mol = molvs.Standardizer().standardize(Chem.MolFromSmiles(smiles))\n",
    "    if mol is None:\n",
    "        return False\n",
    "    else:\n",
    "        return Chem.MolToSmiles(mol)\n",
    "    \n",
    "def set_collect_metric(args):\n",
    "    metric_dict = {'loss':[]}\n",
    "    for metric in args.metric_func:\n",
    "        metric_dict[f'val_{metric}'] = []\n",
    "        metric_dict[f'test_{metric}'] = []\n",
    "    return metric_dict\n",
    "\n",
    "def collect_metric_epoch(collect_metric: dict, loss: float,\n",
    "                         val_scores: dict, test_scores: dict):\n",
    "    collect_metric['loss'].append(loss)\n",
    "    for metric in args.metric_func:\n",
    "        collect_metric[f'val_{metric}'].append(val_scores[metric])\n",
    "        collect_metric[f'test_{metric}'].append(test_scores[metric])\n",
    "    return collect_metric\n",
    "\n",
    "def save_checkpoint(path: str,\n",
    "                    model,\n",
    "                    scaler: StandardScaler = None,\n",
    "                    features_scaler: StandardScaler = None,\n",
    "                    args: Namespace = None):\n",
    "    \"\"\"\n",
    "    Saves a model checkpoint.\n",
    "\n",
    "    :param model: A MoleculeModel.\n",
    "    :param scaler: A StandardScaler fitted on the data.\n",
    "    :param features_scaler: A StandardScaler fitted on the features.\n",
    "    :param args: Arguments namespace.\n",
    "    :param path: Path where checkpoint will be saved.\n",
    "    \"\"\"\n",
    "    state = {\n",
    "        'args': args,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'data_scaler': {\n",
    "            'means': scaler.means,\n",
    "            'stds': scaler.stds\n",
    "        } if scaler is not None else None,\n",
    "        'features_scaler': {\n",
    "            'means': features_scaler.means,\n",
    "            'stds': features_scaler.stds\n",
    "        } if features_scaler is not None else None\n",
    "    }\n",
    "    torch.save(state, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "794it [00:00, 199191.18it/s]\n",
      "100%|██████████| 794/794 [00:00<00:00, 398358.54it/s]\n",
      "100%|██████████| 794/794 [00:00<00:00, 265572.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size: 794,Train size: 633, Val size: 161, Test size: 161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "args.data_name = args.data_path.split('/')[-1].split('.')[0]\n",
    "args.smiles_columns = ['smiles']\n",
    "args.target_columns = ['y']\n",
    "# args.target_columns = ['p_np']\n",
    "\n",
    "df = pd.read_csv(args.data_path)\n",
    "df[args.smiles_columns] = df[args.smiles_columns].applymap(check_molecule)\n",
    "df = df.dropna(subset=args.smiles_columns)\n",
    "\n",
    "set_seed(args.seed)\n",
    "set_save_dir(args)\n",
    "\n",
    "# train = random.sample(list(df.index), int(len(df)*0.8))\n",
    "# df['split'] = 'test'\n",
    "# df.loc[train, 'split'] = 'train'\n",
    "# args.ignore_columns = None\n",
    "\n",
    "# get split data and calculate the activity cliff based on MoleculeACE\n",
    "if 'split' not in df.columns and 'cliff_mol' not in df.columns:\n",
    "    df = split_data(df[args.smiles_columns].values,\n",
    "                    bioactivity=df[args.target_columns].values,\n",
    "                    in_log10=True, similarity=0.9, test_size=0.2, random_state=args.seed)\n",
    "    df.to_csv(args.data_path, index=False)\n",
    "    args.ignore_columns = ['exp_mean [nM]', 'split', 'cliff_mol']\n",
    "else:\n",
    "    args.ignore_columns = None\n",
    "\n",
    "# get data from csv file\n",
    "args.task_names = get_task_names(args.data_path, args.smiles_columns,\n",
    "                                 args.target_columns, args.ignore_columns)\n",
    "data = get_data(path=args.data_path, \n",
    "                smiles_columns=args.smiles_columns,\n",
    "                target_columns=args.target_columns,\n",
    "                ignore_columns=args.ignore_columns)\n",
    "\n",
    "train_idx, test_idx = df[df['split']=='train'].index, df[df['split']=='test'].index\n",
    "val_idx = random.sample(list(train_idx), int(len(test_idx)))\n",
    "\n",
    "train_data, val_data, test_data = tuple([[data[i] for i in train_idx],\n",
    "                                        [data[i] for i in val_idx],\n",
    "                                        [data[i] for i in test_idx]])\n",
    "train_data, val_data, test_data = MoleculeDataset(train_data), \\\n",
    "                                  MoleculeDataset(val_data), \\\n",
    "                                  MoleculeDataset(test_data)\n",
    "\n",
    "if args.features_scaling:\n",
    "    features_scaler = train_data.normalize_features(replace_nan_token=0)\n",
    "    val_data.normalize_features(features_scaler)\n",
    "    test_data.normalize_features(features_scaler)\n",
    "else:\n",
    "    features_scaler = None\n",
    "\n",
    "print(f'Total size: {len(data):,},'\n",
    "      f'Train size: {len(train_data):,}, '\n",
    "      f'Val size: {len(val_data):,}, '\n",
    "      f'Test size: {len(test_data):,}')\n",
    "\n",
    "if args.dataset_type == 'regression':\n",
    "    train_smiles, train_targets = train_data.smiles(), train_data.targets()\n",
    "    scaler = StandardScaler().fit(train_targets)\n",
    "    scaled_targets = scaler.transform(train_targets).tolist()\n",
    "    train_data.set_targets(scaled_targets)\n",
    "\n",
    "else:\n",
    "    # get class sizes for classification\n",
    "    get_class_sizes(data)\n",
    "    scaler = None\n",
    "\n",
    "args.num_tasks = len(args.task_names)\n",
    "\n",
    "model = build_model(args, encoder_name=args.encoder_name)\n",
    "if args.checkpoint_path is not None:\n",
    "    model.encoder.load_state_dict(torch.load(args.checkpoint_path, map_location='cpu'), strict=False)\n",
    "if args.step == 'functional_prompt':\n",
    "    add_functional_prompt(model, args)\n",
    "if args.cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "# Optimizers\n",
    "optimizer = build_optimizer(model, args)\n",
    "\n",
    "# Learning rate schedulers\n",
    "args.train_data_size = len(train_data)\n",
    "scheduler = build_lr_scheduler(optimizer, args)\n",
    "\n",
    "# Loss function\n",
    "loss_func = build_loss_func(args)\n",
    "\n",
    "args.metric_func = ['rmse', 'r2', 'mse']\n",
    "# args.metric_func = ['auc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "330"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df['cliff_mol']==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch:  0\n",
      "Epoch : 00, Training Loss : 2.0623, Validation score : 1.4718, Test score : 1.4785\n",
      "Best model saved at epoch : 00, Validation score : 1.4718\n",
      "Current epoch:  1\n",
      "Epoch : 01, Training Loss : 2.0272, Validation score : 2.2417, Test score : 1.6129\n"
     ]
    }
   ],
   "source": [
    "n_iter = 0\n",
    "args.epochs = 2\n",
    "args.prompt = False\n",
    "metric_dict = set_collect_metric(args)\n",
    "best_score = float('inf') if args.minimize_score else -float('inf')\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    print('Current epoch: ', epoch)\n",
    "    n_iter, loss = train_epoch(args, model, train_data, loss_func, optimizer, scheduler, n_iter)\n",
    "\n",
    "    if isinstance(scheduler, ExponentialLR):\n",
    "        scheduler.step()\n",
    "    val_scores = evaluate_epoch(args, model, val_data, scaler)\n",
    "\n",
    "    test_pred = predict_epoch(args, model, test_data, scaler)\n",
    "    test_scores = evaluate_predictions(test_pred, test_data.targets(),\n",
    "                                       args.num_tasks, args.metric_func, args.dataset_type)\n",
    "    \n",
    "    print('Epoch : {:02d}, Training Loss : {:.4f}, ' \\\n",
    "          'Validation score : {:.4f}, Test score : {:.4f}'.format(epoch, loss,\n",
    "                                 list(val_scores.values())[0][0], list(test_scores.values())[0][0]))\n",
    "    metric_dict = collect_metric_epoch(metric_dict, loss, val_scores, test_scores)\n",
    "    \n",
    "    if args.minimize_score and list(val_scores.values())[0][0] < best_score or \\\n",
    "            not args.minimize_score and list(val_scores.values())[0][0] > best_score:\n",
    "        best_score, best_epoch = list(val_scores.values())[0][0], epoch\n",
    "        best_test_score = list(test_scores.values())[0][0]\n",
    "        save_checkpoint(os.path.join(args.save_dir, 'model.pt'), model, scaler, features_scaler, args) \n",
    "        print('Best model saved at epoch : {:02d}, Validation score : {:.4f}'.format(best_epoch, best_score))\n",
    "print('Final best performed model in {} epoch, val score: {:.4f}, test score: {:.4f}'.format(best_epoch, best_score, best_test_score))\n",
    "\n",
    "# save results\n",
    "pickle.dump(metric_dict, open(os.path.join(args.save_dir, 'metric_dict.pkl'), 'wb'))\n",
    "df['Prediction'] = None\n",
    "df.loc[test_idx, 'Prediction'] = test_pred\n",
    "df[df['split']=='test'].to_csv(os.path.join(args.save_dir, 'test_pred.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cheminfo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
